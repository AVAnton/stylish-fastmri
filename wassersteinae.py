# -*- coding: utf-8 -*-
"""WassersteinAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XbWIPEnv60hV2pHuCyzRHreqNzQ38aEA?usp=sharing
"""

from abc import abstractmethod, ABCMeta
from torch import nn
import torch.nn.functional as F

class Encoder(metaclass=ABCMeta):
    def __init__(self):
        super(Encoder, self).__init__()
        pass

    @abstractmethod
    def forward(self, x):
        pass


class Decoder(metaclass=ABCMeta):
    def __init__(self):
        super(Decoder, self).__init__()
        pass

    @abstractmethod
    def forward(self, x):
        pass


class Bottleneck(metaclass=ABCMeta):
    def __init__(self):
        super(Bottleneck, self).__init__()
        pass

    @abstractmethod
    def forward(self, **kwargs):
        pass


class LossFunction(nn.Module, metaclass=ABCMeta):
    def __init__(self):
        super(LossFunction, self).__init__()

    @abstractmethod
    def __call__(self, *args, **kwargs):
        pass


class View(nn.Module):
    def __init__(self, size):
        super(View, self).__init__()
        self.size = size

    def forward(self, tensor):
        return tensor.view(self.size)

class WassersteinAEncoder(Encoder, nn.Module):
    def __init__(self, z_dim, nc):
        super(WassersteinAEncoder, self).__init__()
        self.z_dim = z_dim
        self.nc = nc
        self.encoder = nn.Sequential(
            nn.Conv2d(nc, 32, 4, 2, 1),  # B,  32, 32, 32
            nn.ReLU(True),
            nn.Conv2d(32, 32, 4, 2, 1),  # B,  32, 16, 16
            nn.ReLU(True),
            nn.Conv2d(32, 32, 4, 2, 1),  # B,  32,  8,  8
            nn.ReLU(True),
            nn.Conv2d(32, 32, 4, 2, 1),  # B,  32,  8,  8
            nn.ReLU(True),
            View((-1, 32 * 8 * 8)),  # B, 2048
            nn.Linear(32 * 8 * 8, 512),  # B, 512
            nn.ReLU(True),
            nn.Linear(512, 256),  # B, 256
            nn.ReLU(True),
            nn.Linear(256, z_dim),  # B, z_dim*2
        )

    def forward(self, x):
        x = self.encoder(x)
        return x

def reconstruction_loss(x, x_recon, distribution='gaussian'):
    batch_size = x.shape[0]
    assert batch_size != 0

    if distribution == 'bernoulli':
        recon_loss = F.binary_cross_entropy_with_logits(x_recon, x, reduction='sum').div(batch_size)
    elif distribution == 'gaussian':
        recon_loss = F.mse_loss(x_recon, x, reduction='sum').div(batch_size)
    else:
        raise ValueError('value error for `distribution` expected: {bernoulli, or gaussian}')

    return recon_loss

class WassersteinADecoder(Decoder, nn.Module):
    def __init__(self, z_dim, nc, target_size):
        super(WassersteinADecoder, self).__init__()
        self.z_dim = z_dim
        self.nc = nc
        self.target_size = target_size

        self.decoder = nn.Sequential(
            nn.Linear(z_dim, 256),  # B, 256
            nn.ReLU(True),
            nn.Linear(256, 256),  # B, 256
            nn.ReLU(True),
            nn.Linear(256, 32 * 8 * 8),  # B, 2048
            nn.ReLU(True),
            View((-1, 32, 8, 8)),  # B,  32,  8,  8
            nn.ConvTranspose2d(32, 32, 4, 2, 1),  # B,  32,  8,  8
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 32, 4, 2, 1),  # B,  32, 16, 16
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 32, 4, 2, 1),  # B,  32, 32, 32
            nn.ReLU(True),
            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B,  nc, 64, 64
            nn.Tanh(),
            View(self.target_size),
        )

    def forward(self, x):
        x = self.decoder(x)
        return x


class WassersteinAE(nn.Module):
    def __init__(self, z_dim, nc, target_size):
        super(WassersteinAE, self).__init__()
        self.encoder = WassersteinAEncoder(z_dim, nc)
        self.decoder = WassersteinADecoder(z_dim, nc, target_size)

    def forward(self, x):
        z = self.encoder(x)
        x = self.decoder(z)
        return x, z


# loss aux functions
def calc_kernel(x_1, x_2, eps=1e-7, z_var=2.):
    x_1 = x_1.unsqueeze(-2)  # Make it into a column tensor
    x_2 = x_2.unsqueeze(-3)  # Make it into a row tensor

    z_dim = x_2.size(-1)
    C = 2 * z_dim * z_var
    kernel = C / (eps + C + (x_1 - x_2).pow(2).sum(dim=-1))

    # Exclude diagonal elements
    result = kernel.sum() - kernel.diag().sum()
    return result


def calc_mmd(z, reg_weight):
    prior_z = torch.rand_like(z)
    prior_z_kernel = calc_kernel(prior_z, prior_z)
    z_kernel = calc_kernel(z, z)
    prior_z_kernel = calc_kernel(prior_z, z)
    mmd = reg_weight * prior_z_kernel.mean() + reg_weight * z_kernel.mean() - 2 * reg_weight * prior_z_kernel.mean()
    return mmd


class WassersteinAELossFunction(LossFunction):
    def __init__(self, reg_weight):
        super().__init__()
        self.reg_weight = reg_weight

    def __call__(self, x, x_recon, z):
        batch_size = x.shape[0]
        bias_corr = batch_size * (batch_size - 1)
        self.reg_weight /= bias_corr

        recon_loss = reconstruction_loss(x_recon, x)
        mmd_loss = calc_mmd(z, self.reg_weight)

        loss = recon_loss + mmd_loss

        return loss

